{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation\n",
    "# TODO modify text description of the steps\n",
    "\n",
    "We will use a dataset consisting of baby product reviews on Amazon.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"../valt_sa_data/amazon_baby.csv\")[['review', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word count vector for each review\n",
    "\n",
    "Let us explore a specific example of a baby product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review    This has been an easy way for my nanny to reco...\n",
       "rating                                                    4\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using Python's built-in string functionality.\n",
    "2. Transform the reviews into word-counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO add more\n",
    "emoticons = [':)', ':))', ':)))', ':(', ':((', ':(((']\n",
    "\n",
    "def extract_emoticons(text):\n",
    "    emoticons_in_text = []\n",
    "    for emoticon in emoticons:\n",
    "        i = text.find(emoticon)\n",
    "        if i > -1:\n",
    "            emoticons_in_text.append(emoticon)\n",
    "    return emoticons_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation_to_remove = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(None, punctuation_to_remove) \n",
    "\n",
    "# TODO add more entries\n",
    "pos_dict = {'NN': 'n', 'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v'}\n",
    "\n",
    "def get_pos_for_lematirzer(brown_post):\n",
    "    if not brown_post in pos_dict:\n",
    "        return 'n'\n",
    "    else:\n",
    "        return pos_dict[brown_post]\n",
    "\n",
    "    \n",
    "# TODO modify and use this list\n",
    "# Or just ignore it\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    " 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    " 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    " 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    " 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    " 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    " 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    " 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    " 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    " 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', \n",
    " 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
    " 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', \n",
    " 'will', 'just', 'don', 'should', 'now']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', \"n't\", \"'s\", 'baby', 'get', 'one', 'bottle', 'would', 'buy', 'love', 'great', 'like', 'time', u'month', 'work', 'son', u'make', u'go', 'old', 'diaper', 'product', 'seat', 'easy', 'little', 'tub', 'also', u'take', 'much', 'first', 'well', 'daughter', 'put', u'try', 'really', u'keep', 'pump', 'think', 'even', u'find', 'still', 'need', 'recommend', 'could', 'back', 'good', 'thing', u'bag', 'problem', 'child', 'purchase', 'toy', u'come', '2', 'year', 'gate', u'look', 'fit', 'give', 'nipple', 'two', u'want', 'sleep', 'since', 'potty', 'play', 'around', 'pillow', 'clean', \"'ve\", u'say', 'way', 'water', \"'m\", 'enough', 'side', 'sit', 'day', 'never', 'better', 'hold', 'every', u'know', 'leak', u'start', '3', 'car', 'change', u'week', 'night', 'best', 'big', u'seem', 'without', 'bath', 'right', 'open', 'room', 'new', '4', 'money', 'see', 'help', 'another', u'lot', 'wash', 'ca', 'bottom', 'small', 'able', u'hand', 'milk', 'avent', 'comfortable', 'size', 'place', 'nice', 'hard', 'item', u'wipe', 'many', 'part', 'worth', 'price', 'perfect', 'top', 'different', '5', 'cover', 'long', 'turn', 'sure', u'set', 'plastic', u'u', 'however', 'pad', 'highly', 'easily', u'sound', 'warmer', 'chair', 'smell', u'read', 'store', 'monitor', u'kid', 'though', 'second', 'something', u'cup', '6', 'happy', 'end', 'husband', 'warm', 'gift', \"'re\", u'move', 'bit', u'feel', 'always', 'crib', 'breast', 'stay', 'last', 'actually', 'far', u'minute', u'piece', 'house', 'easier', 'pretty', 'star', 'design', 'table', 'home', 'friend', 'newborn', 'away', 'extra', 'sling', '1', u'color', 'boy', u'le', 'head', 'wish', 'music', 'pull', 'break', 'almost', 'regular', 'infant', 'soft', u'order', 'definitely', u'tell', u'toddler', 'leave', 'receive', u'decide', 'champ', \"'d\", \"'ll\", 'pail', 'mom', 'fall', 'let', 'stick', 'genie', 'yet', 'anything', 'quality', 'bed', 'food', 'ever', 'must', 'light', 'brand', 'toilet', 'large', u'shower', 'save', u'return', 'difficult', 'dry', 'stroller', 'support', 'fine', 'hear', 'bad', 'anyone', u'bear', 'carry', u'sheet', 'may', 'inside', 'especially', 'safe', 'several', 'waste', 'couple', 'everything', 'people', 'wonderful', u'parent', 'fact', 'door', 'odor', 'throw', 'older', 'spend', 'system', 'instead', 'plus', u'push', 'together', 'foot', 'sometimes', 'close', 'pack', u'strap', 'longer', 'quite', 'sturdy', 'pain', 'refill', 'either', 'wo', 'carrier', 'expensive', 'figure', 'three', u'enjoy', u'slide', u'base', 'run', 'high', 'air', u'worry', 'battery', '12', 'next', u'issue', '8', 'sink', 'formula', 'lid', 'travel', u'box', u'wear', 'dr', u'remove', 'others', 'free', 'playtex', 'brown', 'ago', 'position', 'reason', 'floor', 'full', u'space', '10', 'probably', 'medela', 'allow', 'install', \"'\", 'stop', 'replace', 'eat', 'kitchen', 'else', 'safety', u'switch', 'fill', 'already', 'blanket', u'hour', 'pregnant', 'quickly', 'hot', 'whole', u'handle', 'perfectly', 'hole', 'gas', 'simple', 'nothing', 'secure', u'girl', 'glad', 'absolutely', 'latch', 'stand', 'hang', 'etc', 'idea', u'feature', 'shape', 'walk', 'add', 'grow', u'feed', 'wall', 'become', 'noise', 'least', 'flow', u'expect', u'lock', 'suction', 'complaint', u'ring', u'fold', 'although', u'slip', u'leg', 'maybe', 'fast', 'cold', 'amount', 'front', 'overall', 'liner', 'roll', u'snap', 'nurse', 'cause', 'finally', '9', 'trash', 'might', 'attach', 'feeding', 'half', 'heat', u'learn', 'area', 'bathroom', 'bigger', u'kind', u'face', 'completely', 'case', u'step', 'cute', 'due', 'twin', 'mine', 'trouble', u'begin', 'cloth', 'show', u'instruction', 'mobile', 'smaller', 'oz', 'fun', 'picture', 'life', u'notice', 'adjust', 'deal', u'watch', u'lay', 'middle', 'wide', 'experience', '7', 'bathtub', u'screw', 'mother', u'breastfeed', 'job', u'happen', 'empty', 'everyone', 'pregnancy', 'favorite', 'ready', 'often', 'cost', 'point', u'plug', u'pick', u'cry', u'wake', 'splash', 'arm', 'plan', 'booster', u'button', 'loud', 'mind', u'register', 'later', u'call', 'electric', 'book', 'unit', 'soon', 'cheap', 'drop', 'guard', 'rather', u'mean', 'wait', 'reach', 'larger', u'instal', 'live', 'yes', 'pay', 'spit', 'suppose', u'trip', 'white', 'dirty', 'family', 'amazon', u'edge', 'age', 'durable', '20', 'model', 'type', 'body', 'material', 'option', 'replacement', 'extremely', 'cut', 'super', 'huge', 'outside', 'line', 'reviewer', 'realize', 'hurt', 'apart', 'someone', 'heavy', 'rest', 'stage', 'matter', 'guess', 'real', 'neck', 'wet', 'stool', 'bjorn', 'provide', 'hospital', 'nursing', 'along', u'bring', 'dishwasher', 'excellent', 'company', 'convenient', 'four', 'mess', 'pleased', ':)', u'lose', 'pop', 'machine', 'carseat', 'mirror', 'tight', 'storage', 'onto', u'check', u'catch', 'weight', u'fee', 'volume', 'suck', '15', 'simply', 'wife', 'short', 'saw', 'prevent', 'static', 'within', u'suggest', u'us', 'disposable', 'useful', 'level', 'care', 'kick', 'style', 'mattress', 'wrong', 'dont', u'include', 'unless', u'send', 'mat', 'ok', 'constantly', 'anyway', 'seal', 'direction', 'mouth', 'fabric', 'corner', u'pee', 'inch', 'garbage', 'standard', 'otherwise', 'usually', 'clear', 'believe', 'properly', u'choose', 'flat', u'mention', 'prefer', 'control', 'dog', u'train', 'sack', 'hit', 'hope', 'tall', 'course', 'cheaper', 'sell', 'continue', 'asleep', 'consider', 'nursery', 'low', u'double', 'tray', 'height', 'drain', 'uncomfortable', 'tip', 'ask', 'disappointed', u'compare', 'entire', '1st', 'purpose', 'dekor', 'protect', 'awesome', u'arrive', 'stuff', 'past', u'stair', u'hat', 'quick', 'follow', 'research', u'spill', 'grandson', 'felt', u'annoy', 'helpful', 'slow', 'boppy', 'strong', 'immediately', 'tiny', 'require', 'version', 'foam', 'anymore', 'recently', 'graco', u'cap', u'cube', u'outgrow', 'imagine', 'thin', 'market', u'assemble', 'bar', 'r', 'complain', 'bulky', 'special', 'grab', 'forward', 'flip', 'touch', '18', 'plenty', 'handy', u'solid', 'twice', 'nicely', 'early', 'amp', 'difference', u'finger', 'snoogle', 'training', 'agree', 'blue', u'number', 'young', 'adult', 'rubber', 'opening', 'walker', 'thick', 'alot', 'exactly', 'contain', u'surface', 'burp', 'wood', 'sippy', 'wrap', 'nearly', 'please', u'extension', 'bright', 'write', 'impossible', 'online', 'twist', 'toss', 'shield', 'anywhere', u'choice', 'pressure', 'morning', 'totally', u'mo', 'negative', 'belly', u'vent', 'horrible', '30', 'belt', 'customer', 'third', u'eye', 'climb', 'evenflo', 'temperature', 'drink', 'saver', 'manual', u'match', 'insert', u'scream', 'power', 'chew', 'isis', 'correctly', 'comfort', 'bather', 'bowl', u'note', 'stink', u'kit', u'lb', 'everywhere', u'test', u'guy', 'fell', 'terrible', u'ounce', 'safer', 'package', 'counter', u'crack', 'sister', 'across', u'hop', 'straight', 'five', 'cotton', 'gerber', 'summer', 'shut', 'portable', u'build', 'single', 'tie', u'fix', 'ease', u'knee', u'frame', 'daily', 'important', 'christmas', 'spoon', 'press', 'originally', 'tummy', 'except', u'comment', 'n', u'listen', 'birth', 'tape', 'clothes', u'microwave', 'ventaire', 'container', u'talk', 'hook', 'forget', 'comfy', 'six', 'pedal', 'babies', u'tend', u'near', 'underneath', 'bought', 'sterilizer', 'sponge', 'today', 'unfortunately', 'towel', 'available', 'shop', 'cord', 'bc', 'bother', 'loose', 'state', u'shoulder', 'service', 'luck', u'pour', 'nap', 'manufacturer', 'comfortably', 'tighten', 'cool', 'glass', u'block', u'cushion', 'lift', 'person', u'wheel', 'remember', 'finish', '11', u'sterilize', 'yard', 'highchair', u'swing', 'zipper', 'bathe', 'loves', 'phone', 'rip', 'colic', u'damage', u'crawl', u'supply', 'mechanism', 'ride', 'pacifier', 'attention', 'lean', 'playpen', 'video', u'transition', 'thank', 'restaurant', 'understand', 'alone', u'pound', 'spray', 'crazy', u'device', 'halo', 'poor', 'reflux', 'careful', 'slightly', u'offer', 'tear', 'truly', 'similar', 'mount', 'unlike', u'pocket', u'soak', 'hassle', 'fantastic', u'create', 'woman', 'instrument', 'registry', 'sleeper', 'pink', 'normal', u'strip', 'lower', 'sort', 'velcro', 'spot', 'everytime', 'yr', u'surprise', 'hate', 'investment', u'concern', u'poop', 'useless', '23', 'rinse', u'search', 'fairly', 'barely', 'thanks', 'avoid', 'wonder', 'behind', 'fisher', u'drive', 'ball', 'contact', u'shake', 'mark', u'recline', 'grandma', 'breastmilk', 'true', 'interested', 'cleaning', 'higher', u'visit', 'worst', 'hip', 'happier', 'duck', u'knob', 'result', 'cd', 'penny', '100', 'adhesive', 'song', u'boil', 'local', 'possible', 'original', 'currently', 'green', 'skin', 'express', 'red', 'center', 'doesnt', u'serve', 'cabinet', 'basically', 'mommy', 'natural', 'upstairs', 'opinion', 'bend', 'bra', 'eventually', 'securely', 'lie', 'key', 'addition', u'bib', 'worse', u'mix', u'discover', u'bubble', u'buckle', 'inexpensive', 'faster', 'okay', 'browns', 'removable', 'stack', u'disappoint', 'solution', 'bassinet', 'didnt', u'rail', 'ton', 'metal', u'separate', 'afraid', u'pass', 'process', 'mistake', 'years', 'basket', 'compartment', 'bunch', 'multiple', 'snug', '2nd', u'smile', 'bedding', 'directly', u'miss', 'washing', 'breastfed', u'accident', 'outlet', 'bumper', 'bedroom', 'compact', u'teethe', 'frequently', 'cant', u'depend', u'amaze', 'main', 'everyday', u'name', 'holder', 'quiet', u'list', 'beautiful', u'reduce', 'chance', 'leather', 'choke', 'awful', 'clip', 'effective', 'player', u'stain', 'nurser', 'gym', '14', 'mesh', u'brush', 'exact', 'bump', 'musical', 'certainly', u'manage', 'harder', 'colorful', 'spout', 'dish', u'tab', 'definately', 'whether', 'busy', 'tube', u'ship', 'forth']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "sentence = \"I am a big boy:) I'd love to eat ice-cream right now, and my friend goes shopping. By the way, Danylo already went.\"\n",
    "\n",
    "my_words_list = []\n",
    "my_words = set()\n",
    "\n",
    "def analyze_review(text):\n",
    "    global my_words\n",
    "    emoticons_features = extract_emoticons(text)\n",
    "    text_without_punctuation = remove_punctuation(text)\n",
    "    tokens = nltk.word_tokenize(text_without_punctuation)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # TODO apply Turney alorithm\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens_prepared_for_lemmatization = [(t[0], get_pos_for_lematirzer(t[1])) for t in tagged_tokens]\n",
    "    lemmas = [lemmatizer.lemmatize(tpl[0], tpl[1]) for tpl in tokens_prepared_for_lemmatization]\n",
    "    \n",
    "    filtered_lemmas = []\n",
    "    for lemma in lemmas:\n",
    "        lemma_l = lemma.lower()\n",
    "        if not lemma_l in stop_words:\n",
    "            filtered_lemmas.append(lemma_l)\n",
    "    \n",
    "    words = filtered_lemmas + emoticons_features\n",
    "    for word in words:\n",
    "        if not word in ['review', 'rating']:\n",
    "            my_words_list.append(word)\n",
    "            my_words.add(word)\n",
    "    return words\n",
    "\n",
    "# analyze_review(sentence)\n",
    "\n",
    "analyzed_reviews = products['review'].apply(str).apply(analyze_review)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "count = Counter(my_words_list)\n",
    "#print(\"after: len(count) = %s\") %(len(count))\n",
    "#print(\"most_common = %s\") %(count.most_common(1000))\n",
    "\n",
    "most_common_words = map(lambda x: x[0], count.most_common(1000))\n",
    "\n",
    "print most_common_words\n",
    "#TODO apply lower in the right moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21740\n"
     ]
    }
   ],
   "source": [
    "print len(my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "#       'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "#       'work', 'product', 'money', 'would', 'return']\n",
    "\n",
    "significant_words = most_common_words #+ emoticons # list(my_words) #[0:500]\n",
    "        \n",
    "def count_number_of_significant_words(text):\n",
    "    words = text['review']\n",
    "    word_dict = {}\n",
    "    for word in significant_words:\n",
    "        word_dict[word] = 0\n",
    "    for word in words:\n",
    "        if word in significant_words:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "                #word_dict[word] = word_dict[word] + 1\n",
    "                #pass\n",
    "    significant_words_counts = []\n",
    "    for word in significant_words:\n",
    "        significant_words_counts.append(word_dict[word]) \n",
    "    return pd.Series(significant_words_counts, index=significant_words)\n",
    "\n",
    "\n",
    "newcols = pd.DataFrame(analyzed_reviews).apply(count_number_of_significant_words, axis=1)\n",
    "newcols.columns = significant_words\n",
    "\n",
    "products_with_words = products.join(newcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explore what the sample example above looks like after these 2 transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review        This has been an easy way for my nanny to reco...\n",
       "rating                                                        4\n",
       "use                                                           0\n",
       "n't                                                           1\n",
       "'s                                                            0\n",
       "baby                                                          1\n",
       "get                                                           0\n",
       "one                                                           1\n",
       "bottle                                                        0\n",
       "would                                                         1\n",
       "buy                                                           0\n",
       "love                                                          0\n",
       "great                                                         0\n",
       "like                                                          0\n",
       "time                                                          0\n",
       "month                                                         0\n",
       "work                                                          0\n",
       "son                                                           0\n",
       "make                                                          0\n",
       "go                                                            0\n",
       "old                                                           0\n",
       "diaper                                                        0\n",
       "product                                                       0\n",
       "seat                                                          0\n",
       "easy                                                          1\n",
       "little                                                        0\n",
       "tub                                                           0\n",
       "also                                                          0\n",
       "take                                                          0\n",
       "much                                                          0\n",
       "                                    ...                        \n",
       "reduce                                                        0\n",
       "chance                                                        0\n",
       "leather                                                       0\n",
       "choke                                                         0\n",
       "awful                                                         0\n",
       "clip                                                          0\n",
       "effective                                                     0\n",
       "player                                                        0\n",
       "stain                                                         0\n",
       "nurser                                                        0\n",
       "gym                                                           0\n",
       "14                                                            0\n",
       "mesh                                                          0\n",
       "brush                                                         0\n",
       "exact                                                         0\n",
       "bump                                                          0\n",
       "musical                                                       0\n",
       "certainly                                                     0\n",
       "manage                                                        0\n",
       "harder                                                        0\n",
       "colorful                                                      0\n",
       "spout                                                         0\n",
       "dish                                                          0\n",
       "tab                                                           0\n",
       "definately                                                    0\n",
       "whether                                                       0\n",
       "busy                                                          0\n",
       "tube                                                          0\n",
       "ship                                                          0\n",
       "forth                                                         0\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_with_words.iloc[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prepared data into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = products_with_words[significant_words]\n",
    "y = products_with_words['rating']\n",
    "X.to_csv('../valt_sa_data/x_m.csv', index=False)\n",
    "y.to_csv('../valt_sa_data/y_m.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
