{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation\n",
    "\n",
    "We will use a dataset consisting of baby product reviews on Amazon.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_stemming = True\n",
    "use_l = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"../valt_sa_data/amazon_baby.csv\")[['review', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word count vector for each review\n",
    "\n",
    "Let us explore a specific example of a baby product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review    This has been an easy way for my nanny to reco...\n",
       "rating                                                    4\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using Python's built-in string functionality.\n",
    "2. Transform the reviews into word-counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticons = [':)', ':))', ':(']\n",
    "# def extract_emoticons(text):\n",
    "#     em_dict = {}\n",
    "#     for emoticon in emoticons:\n",
    "#         i = text.find(emoticon)\n",
    "#         if i == -1:\n",
    "#             em_dict[emoticon] = 0\n",
    "#         else:\n",
    "#             em_dict[emoticon] = 1\n",
    "#     return em_dict\n",
    "\n",
    "def extract_emoticons(text):\n",
    "    emoticons_in_text = []\n",
    "    for emoticon in emoticons:\n",
    "        i = text.find(emoticon)\n",
    "        if i > -1:\n",
    "            emoticons_in_text.append(emoticon)\n",
    "    return emoticons_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation_to_remove = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(None, punctuation_to_remove) \n",
    "\n",
    "# TODO add more entries\n",
    "pos_dict = {'NN': 'n', 'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v'}\n",
    "\n",
    "def get_pos_for_lematirzer(brown_post):\n",
    "    if not brown_post in pos_dict:\n",
    "        return 'n'\n",
    "    else:\n",
    "        return pos_dict[brown_post]\n",
    "\n",
    "    \n",
    "    \n",
    "# TODO modify and use this list\n",
    "s = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    " 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    " 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    " 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    " 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    " 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    " 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    " 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    " 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    " 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', \n",
    " 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
    " 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', \n",
    " 'will', 'just', 'don', 'should', 'now']    \n",
    "    \n",
    "#my_bigrams = nltk.bigrams(tokens)\n",
    "#my_trigrams = nltk.trigrams(tokens)\n",
    "#for bigram in my_bigrams:\n",
    "#    print bigram\n",
    "#for trigram in my_trigrams:\n",
    "#    print trigram\n",
    "#print type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " u'be',\n",
       " 'a',\n",
       " 'big',\n",
       " 'boy',\n",
       " 'I',\n",
       " \"'d\",\n",
       " 'love',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'icecream',\n",
       " 'right',\n",
       " 'now',\n",
       " 'and',\n",
       " 'my',\n",
       " 'friend',\n",
       " u'go',\n",
       " 'shopping',\n",
       " 'By',\n",
       " 'the',\n",
       " 'way',\n",
       " 'Danylo',\n",
       " 'already',\n",
       " u'go',\n",
       " ':)']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "sentence = \"I am a big boy:) I'd love to eat ice-cream right now, and my friend goes shopping. By the way, Danylo already went.\"\n",
    "\n",
    "def analyze_text(text):\n",
    "    emoticons_features = extract_emoticons(text)\n",
    "    text_without_punctuation = remove_punctuation(text)\n",
    "    tokens = nltk.word_tokenize(text_without_punctuation)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # TODO apply Turney alorithm\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens_prepared_for_lemmatization = [(t[0], get_pos_for_lematirzer(t[1])) for t in tagged_tokens]\n",
    "    lemmas = [lemmatizer.lemmatize(tpl[0], tpl[1]) for tpl in tokens_prepared_for_lemmatization]\n",
    "    return lemmas + emoticons_features\n",
    "\n",
    "analyze_text(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "sentence = \"I am a big boy:) I'd love to eat ice-cream right now, and my friend goes shopping. By the way, Danylo already went.\"\n",
    "\n",
    "#print extract_emoticons(sentence)\n",
    "\n",
    "sentence_without_punctuation = remove_punctuation(sentence)\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence_without_punctuation)\n",
    "#print tokens\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "#print tagged\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "prepared_for_l = [(t, get_pos_for_lematirzer(t[1])) for t in tagged]\n",
    "\n",
    "#print prepared_for_l\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(pl[0][0], pl[1]) for pl in prepared_for_l]\n",
    "\n",
    "count = Counter(lemmas)\n",
    "\n",
    "#print count\n",
    "\n",
    "#print(\"len(count) = %s\") % (len(count))\n",
    "#print(\"most_common = %s\") % (count.most_common(10))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "#print vectorizer \n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.', \n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#print vectorizer.get_feature_names() \n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_without_puctuation = products['review'].apply(str).apply(remove_punctuation)\n",
    "\n",
    "import nltk\n",
    "\n",
    "my_words = set()\n",
    "\n",
    "def my_split(text):\n",
    "    words = []\n",
    "    global my_words\n",
    "    mw = nltk.word_tokenize(text)\n",
    "    for w in mw:\n",
    "        if w != 'rating':\n",
    "            words.append(w)\n",
    "            my_words.add(w)\n",
    "    return words\n",
    "\n",
    "#TODO nltk stemming\n",
    "\n",
    "#review_without_puctuation_new = review_without_puctuation\n",
    "\n",
    "review_without_puctuation_new = review_without_puctuation.apply(my_split)\n",
    "\n",
    "# print my_words\n",
    "\n",
    "# review_without_puctuation = products['review'].apply(str).apply(my_split)\n",
    "\n",
    "#import nltk\n",
    "# sentence = \"I am a big boy:) I'd love to eat ice-cream right now, and my friend goes shopping. By the way, Danylo already went.\"\n",
    "# nltk.word_tokenize\n",
    "# tokens = nltk.word_tokenize(sentence)\n",
    "# print tokens\n",
    "# print type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "#       'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "#       'work', 'product', 'money', 'would', 'return']\n",
    "\n",
    "significant_words = list(my_words)[1:50]\n",
    "        \n",
    "def count_number_of_significant_words(text):\n",
    "    words = text['review'] #.split()\n",
    "    word_dict = {}\n",
    "    for word in significant_words:\n",
    "        word_dict[word] = 0\n",
    "    for word in words:\n",
    "        if word in significant_words:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 1\n",
    "            else:\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "                #pass\n",
    "    significant_words_counts = []\n",
    "    for word in significant_words:\n",
    "        significant_words_counts.append(word_dict[word]) \n",
    "    return pd.Series(significant_words_counts, index=significant_words)\n",
    "\n",
    "\n",
    "newcols = pd.DataFrame(review_without_puctuation_new).apply(count_number_of_significant_words, axis=1)\n",
    "newcols.columns = significant_words\n",
    "\n",
    "products_with_words = products.join(newcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explore what the sample example above looks like after these 2 transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review           This has been an easy way for my nanny to reco...\n",
       "rating                                                           4\n",
       "6pm                                                              0\n",
       "saved                                                            0\n",
       "feeding                                                          0\n",
       "teaching                                                         0\n",
       "worth                                                            0\n",
       "every                                                            0\n",
       "squares                                                          0\n",
       "solution                                                         0\n",
       "clothes                                                          0\n",
       "enjoy                                                            0\n",
       "chew                                                             0\n",
       "quilt                                                            0\n",
       "tired                                                            0\n",
       "Fortunately                                                      0\n",
       "second                                                           0\n",
       "even                                                             0\n",
       "cooking                                                          0\n",
       "fingers                                                          0\n",
       "Amazon                                                           0\n",
       "new                                                              0\n",
       "centric                                                          0\n",
       "never                                                            0\n",
       "here                                                             0\n",
       "reported                                                         0\n",
       "nanny                                                            1\n",
       "dry                                                              0\n",
       "kids                                                             0\n",
       "daughter                                                         0\n",
       "leaves                                                           0\n",
       "pinning                                                          0\n",
       "NOT                                                              0\n",
       "settled                                                          0\n",
       "fantastic                                                        0\n",
       "highly                                                           1\n",
       "Also                                                             0\n",
       "specifics                                                        0\n",
       "daughterinlaw                                                    0\n",
       "would                                                            0\n",
       "Includes                                                         0\n",
       "music                                                            0\n",
       "recommend                                                        1\n",
       "type                                                             0\n",
       "tell                                                             0\n",
       "haha                                                             0\n",
       "hurt                                                             0\n",
       "phone                                                            0\n",
       "adult                                                            0\n",
       "excellent                                                        0\n",
       "hold                                                             0\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_with_words.iloc[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prepared data into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = products_with_words[significant_words]\n",
    "y = products_with_words['rating']\n",
    "X.to_csv('../valt_sa_data/x_m.csv', index=False)\n",
    "y.to_csv('../valt_sa_data/y_m.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
