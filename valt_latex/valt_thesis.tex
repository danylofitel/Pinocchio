\documentclass[12pt]{report}

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{srcltx}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}

\fancypagestyle{plain}{}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\renewcommand{\headrulewidth}{0 pt}
\rhead{\thepage}
\setlength{\headwidth}{483 pt}
\setlength{\headheight}{0.8 pt}

\paperheight 297mm \paperwidth 220mm \oddsidemargin -7mm
\topmargin 0mm \textwidth 170mm \textheight 220mm \marginparsep
0mm \marginparwidth 0mm \headheight 0mm \headsep 0mm \footskip 5mm
\parskip 2pt

\DeclareMathOperator{\Bool}{Bool}
\newcommand{\Wr}{\mathop{\font\xx=cmsy10 at 18pt \hbox{\xx \char111}}}
\def\thnumbering{chapter}
\def\equa{\Leftrightarrow}
\def\x{{\mathcal{X}}}
\def\b{{\mathcal{B}}}

\newcommand{\bb}[1]{{\bf #1}}
\newcommand{\wsp}{\hspace{6pt}}
\newcommand{\re}{\mathbb{R}}
\newcommand{\Loss}{\mathcal{L}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]

\makeatletter
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}{-\baselineskip}{-5pt}{\bf}}
\makeatother


\begin{document}


\renewcommand{\bibname}{References}
\setcounter{tocdepth}{1}

\setcounter{page}{2}

\large

\thispagestyle{empty}
\tableofcontents

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

In this paper I would like to investigate various methods of text analysis to improve quality of predictions based on text messages. A trade-off between prediction quality and time required is also considered.

Text analysis, which has other names, in particular, text mining and text analytics, is continiously developing. Labor-intensive manual text mining approaches first appeared in the mid-1980s, but technological advances have enabled the field to advance during the past decade. Text mining usually involves the process of structuring the input text, deriving patterns within the structured data, and finally evaluation and interpretation of the output. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and a plethora of others.
The term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual documents for research and business intelligence,

Sentiment analysis, which refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials, is the most crucial problem for businesses related to social media and market advertisement. An average client may produce about 100 messages per day. What is more, sometimes sentiment data is generated by web crawlers which aggregate data from various sources. Given the amount of data those enterprises poses, it is almost infeasible to process all the messages manually. Therefore, automatic text analytics is indispensable and even methods with low precision are acceptable solution.

A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level - whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Advanced, "beyond polarity" sentiment classification looks, for instance, at emotional states such as "angry," "sad," and "happy." In this work only the basic task is considered.

Existing approaches to sentiment analysis can be grouped into three main categories: knowledge-based techniques, statistical methods, and hybrid approaches which combine the aforementioned methods.

Knowledge-based techniques classify text into categories based on the presence of words such as happy, sad, afraid, and bored. Some knowledge bases not only list obvious affect words, but also assign arbitrary words a probable "affinity" to particular emotions. Those techniques can deliver better accuracy because they make use of the domain knowledge. Unfortunately such techniques heavily depend on the domain of the document and as a result the classifiers cannot be ported to other domains.

Statistical methods leverage machine learning and statistics. Those techniques have probabilistic background and focus on the relations between the words and categories. Statistical methods have 2 significant benefits over the Knowledge-based techniques: we can use them in other domains and languages with minor or no adaptations and we can use Machine Translation of the original dataset and still get quite good results. This obviously is impossible by using knowledge-based techniques.

In this paper I consider using statistical methods to perform sentiment analysis of the data refered to a specific domain. I leverage different common approaches and suggest improvements to them for solving my particular task. Moreover, I analyze some of the common problems like feature vector selection. Finally, I provide a performance comparison between my approach and common solutions in practice.

\newpage


\chapter{Introduction}

\section{Sentiment analysis and its applications}

A marketing campaign is efforts of a company or a third-party marketing company to increase awareness for a particular product or service, or to increase consumer awareness of a business or organization. It has a limited duration. A marketing campaign consists of specific activities designed to promote a product, service or business. A marketing campaign is a coordinated series of steps that can include promotion of a product through different mediums (television, radio, print, online) using a variety of different types of advertisements. The campaign doesn't have to rely solely on advertising, and can also include demonstrations, word of mouth and other interactive techniques.

After a particular campaign is finished, the business is interested in aggregating people's opinion about the products which were being promoted. The vast majority of the reviews come from social media and shoping websites where no numerical grades are presented. Basically, a review is a short text which is either positive or negative. As was mentioned before, because of large quantities of customers and review sources, manual review analysis is not feasible. The solution is to use an automated review analysis. A classificator can be built on the data that have been already analysed by humans. The larger the corpus the better.

In general, the problem of sentiment analysis depends heavily on the application's domain and can benefit from additional metadata available along with the text data. For example, in case of reviews related to political domain, the analyst might expect some amount of sarcasm and indirect implications. Moreover, a language which is used also plays important role; similar expressions in different languages have different shades of meaning. Therefore, it is often beneficial to target a specific domain and a specific language in text sentiment analysis problem. In this parer though, I consider applying only statistic methods for detection of domain specific features. There is an evidence that those techniques often produce sufficient performance \cite{turney} and remain generaly applicable.

Most of the solutions focusing on global review classification consider only the polarity of the review (positive/negative) and rely on machine learning techniques. Solutions that aim to create is a more detailed classification of reviews (e.g., three or five star ratings) with usage of linguistic features including negation and modality.

In this paper I am going to target customer reviews of baby products in English available on amazon website, the biggest retailer in the world. The data is available at \cite{amazon-data}.
This data contains 183532 labeled products reviews. The polarity grade is 1-5. 1 - means the most negative review and 5 - the most positive.

\newpage

\section{Definitions}

Let us denote a set of all possible words $W$ and a set of all possible permutations G.
And let $R$ be a set of all the reviews available. So $r \in R$ - product review.
$R = \{(w, g): w \in W, g \in G\}$.
$P$ - review polarities set. Each review has corresponding polarity $p=\{1,2,3,4,5\}, p \in P$.
$|R| = |P| = m$.

$X$ feature matrix for machine learning. $X$ is an $m\times n$ matrix, where $m$ is a number of training samples and $n$ is a feature vector cardinality.

$Y$ - label vector for machine learning. In this case it is a mapped polarity of a review. $Y=\{-1,1\}$, where -1 correspond to negative opinion and 1 - to the positive. $|Y| = m$.

$X_{train} \subseteq X$ - training data.

$X_{test} \subseteq X$ - test data.

$Y_{train} \subseteq Y$ - training labels.

$Y_{test} \subseteq Y$ - test labels.

$extr: R \rightarrow X$ - feature extraction function. Given a review $r$, this functions maps it to the feature vector $x$.

$pol: P \rightarrow Y$ - polarity map function. Maps polarity to binary vector. 

$f: X \rightarrow Y$ - classification function. Given a feature vector $x$, this function predicts its label $y$.

Now text analysis process can be defined. The sets $R$ and $P$ are available.
Firstly, an extraction function $extr$ must be defined. After feature extraction process we should be able to form a feature matrix $X$. Also the representation of the polarities set $P$ is changed by applying a map function $pol$. Hence, $X$ and $Y$ are defined.

At this moment, I shall split the data into training and cross-validation set. Or in other words training and test data. Now $f$ must be found by a classification algorithm on the training set.

The ultimate goal is to obtain a decision function $F : R \rightarrow \{extr, f\}$ that would minimize $\dfrac1 {m_{test}} \sum_{i=1}^{m_{test}} (f(x_{i})- y_{i})^2$ for training data. 


TODO precission and recall
TODO describe F1-score.

\newpage

\chapter{Classification Techniques}

\section{k Nearest Neighbors}

$k$ Nearest Neighbors (KNN) is part of supervised learning that has been used in many applications in the field of data mining, statistical pattern recognition, image processing and many others. The idea is to predict class for a given feature vector using so called majority voting. Suppose we want to predict label for a feature vector $x_i \in X_{test}$. Then we look on labels of $k$ training vectors which are "the closest" to $x_i$ and use the most frequent label as a prediction. To define "closeness" some distance metric is used. There are several options for distance functions.

A commonly used distance metric for continuous and discrete variables is Euclidean distance:
$$ \sum_{i=1}^{n} (a_i - b_i)^2$$, where a,b - feature vectors. $a_i - b_i$ - vector difference.

For discrete variables, such as for text search, another metric can be used, such as the overlap metric (or Hamming distance).

The best choice of k depends upon the data; generally, larger values of k reduce the effect of noise on the classification, but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques. In our case though I am going to use a fixed value.

This alorithm does not require training step. Having sets $X_{train}$ and $Y_{train}$ and distance function $dist$ is enough for making predictions. On the other hand, the classification of test data takes time, because for every review without label, the whole training corpus must be processed to calculate distance. For one prediction expected time complexity is $O(m_{train} * n)$. Performing indexing on the training set can decrease the running closer to $O(n)$. However, indexing is a computationally heavy operation and in case data is updated often, this approach becomes infeasible. Moreover, precission of this classification algorithm is usually low compared to others. Another drawback of the basic "majority voting" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number.

In practice KNN algorithm performs worse than other algorithms. Therefore KNN model is mostly used as a baseline to compare it with other models.

\newpage

\section{Naive Bayes}

Naive Bayes algorithm is a classification algorithm based on Bayes' theorems, and can be used for both exploratory and predictive modeling. The word naive in the name Naive Bayes derives from the fact that the algorithm uses Bayesian techniques but does not take into account dependencies that may exist. This algorithm is less computationally intense than most other classification algorithms, and therefore is useful for quickly generating mining models to discover relationships between input columns and predictable columns %\cite{mic}.

Naive Bayes relies on very simple representation of the document - Bag of words.

Input:

\begin{itemize}
  \item A document $d$
  \item A fixed set of classes $C=\{c_1, c_2,..., c_j\}$
  \item A training set of $m$ hand-labeled documents $(d_1, c_1), (d_2, c_2),..., (d_m, c_m)$
\end{itemize}

Output: 
\begin{itemize}
  \item A learned classifier $y: d \rightarrow c$.
\end{itemize}

According to the Bayes theorem, for a document $d$ and class $c$:

$$P(c | d) = \dfrac{P(d | c) P(c)}{P(d)}$$.

This probability can be used to predict a class of a document.

$$c_{best} = \arg\!\max_{c \in C} P(c | d) = \arg\!\max_{c \in C} \dfrac{P(d | c) P(c)}{P(d)} = \arg\!\max_{c \in C} {P(d | c) P(c)}$$

Taking into account the fact that d is a bag (set) of words:

$$c_{best} = \arg\!\max_{c \in C} {P(d | c) P(c)} = \arg\!\max_{c \in C} {P(w_1, w_2,..., w_n | c) P(c)}$$

Now let us find the way to compute two probailities in the equation above. Let us start with $P(c)$. This probability can be estimated by computing how often this class occurs. It can be computed by counting relative frequencies in a corpus.

Computing $P(w_1, w_2,..., w_n | c)$ is more subtle. There is $|W|^n * |C|$ parameters to be computed, which is impossible in practice. In Naive Bayes classifier two simplifying assumptions are made:

\begin{itemize}
  \item \textbf{Bag of Words assumption:} Assume position does not matter
  \item \textbf{Conditional Independence:} Assume the feature probabilities $P(x_i | c)$ are independent given a class c.
\end{itemize}

Despite these simplification, in practice a problem can be solved with a high degree of accuracy. The result of the simplyfying assumptions is the following equation:
$$P(w_1, w_2,..., w_n | c) = P(w_1 | c) \times P(w_2 | c) \times ... \times P(w_n | c)$$

Therefore:
$$c_{best} = \arg\!\max_{c \in C} {P(w_1, w_2,..., w_n | c) P(c)} = \arg\!\max_{c \in C} P(c) \prod_{w \in W} {P(w | c) }$$

Now we have to find the way to compute these probabilities. The most natural way to implement this is simply to use frequencies in the data.

$$P(w_i | c) = \dfrac{doccount(C=c)}{N_{doc}}$$

$$P(c) = \dfrac{count(w_i, c)}{\sum_{w \in W} {count(w, c)}}$$

So I am going to compute the fraction of times word $w_i$ appears among all words in documents of class $c$. To do that it is possible to concatenate all available documents of a particular class $c$ into one mega-document. In case of this paper there is two mega-docuemtns $D_{pos}$ and $D_{neg}$, which correspond to positive and negative reviews.

Unfortunatelly, there is a problem with the approach described above. What should happen if we have not seen training example with the word \textbf{fantastic} and classified in the positive?
$$P("fantastic" | positive) = \dfrac{count("fantastic", positive)}{\sum_{w \in W} {count(w, positive)}} = 0$$

As can be seen, even though the word \textbf{fantastic} is usually associated with positive sentiment, the probability of a document which contains this word belongs to $D_{pos}$ is zero. Here is why:
$$c_{best} = \arg\!\max_{c \in C} P(c) P("fantastic" | c) \prod_{w \in W/\{"fantastic"\}} {P(w | c) }$$

For positive class this product is equal to zero.

The solution to this problem is adding Laplace (add-1) smoothing for Naive Bayes. Simply one is added to each of the counts:

$$P(w_i | c) = \dfrac{count(w_i, c) + 1}{(\sum_{w \in W} {(count(w, c) + 1)}} = \dfrac{count(w_i, c) + 1}{\sum_{w \in W} {count(w, c)}) + |W|}$$


Now the algorithm can be defined:

\begin{itemize}
  \item From training corpus, extract Vocabulary - the list of words.
  \item Calculate $P(c)$ terms
    
   For each $c_j \in C$ do
  
      $docs_j$ <- all documents with class $c_j$ 

$$P(w_i | c_j) = \dfrac{|docs_j|}{|total \# documents|}$$

  \item Calculate $P(w_k | c_j)$ terms

$Text_j$ <- single document containing all $docs_j$

For each $w_k \in W$

$n_k$ <- number of occurences of $w_k$ in $Text_j$

$n$ <- total number of words in the class $j$.

$$P(w_k | c_j) = \dfrac{n_k + \alpha}{n + \alpha \times |W|}$$

Note: In the last equation $\alpha$ is used instead of one to generalize the model.
\end{itemize}

It turns out Naive Bayes has a very close relationship to language modeling.
Each class in Naive Bayes classifier is a unigram language model. TODO link on unigram language model.

Probaility of word: $P(word | c)$

Probability of sentence: $P(s | c) = \prod{}{P(word | c)}$

Let us see how it works. Imagine that we have a class positive:
And we have table of likelihoods of each word.

\begin{center}
  \begin{tabular}{ c | c }
    \hline
    Likelihood & Word \\ \hline
    0.05 & I \\ \hline
    0.2 & love \\ \hline
    0.002 & this \\ \hline
    0.05 & beautiful \\ \hline
    0.03 & dress \\ \hline
  \end{tabular}
\end{center}

Then the probability of sentence "I love this fun film" will be $P(s | pos) = 0.00000003$.

A model negative can look like:

\begin{center}
  \begin{tabular}{ c | c }
    \hline
    Likelihood & Word \\ \hline
    0.05 & I \\ \hline
    0.002 & love \\ \hline
    0.002 & this \\ \hline
    0.0001 & beautiful \\ \hline
    0.03 & dress \\ \hline
  \end{tabular}
\end{center}

Then the probability of sentence "I love this fun film" will be $P(s | neg) = 0.0000000000006$.

As we can see $P(s | pos) > P(s | neg)$. Therefore, this sentence is classified as positive, which is correct.
Hence, Naive Bayes can be viewed as a unigram language model conditioned on the class.


Summary:
\begin{itemize}
  \item Very fast algorithm, low storage requirements
  \item Robust to irrelevent features: Irrelevant features cancel each other without affecting results.
  \item Very good at domains with many equally important features
  \item Optimal if idependence assumption is true. Of course it is rare.
\end{itemize}

\newpage

\section{Logistic regression}

Linear regression can easily be used for classification in domains with numeric attributes. Indeed, we can use any regression technique, whether linear or nonlinear, for classification. The trick is to perform a regression for each class, setting the output equal to 1 for training instances that belong to the class and 0 for those that do not. The result is a linear expression for the class. Then, given a test example of unknown class, calculate the value of each linear expression and choose the one that is largest. This scheme is sometimes called multiresponse linear regression.

One way of looking at multiresponse linear regression is to imagine that it approximates a numeric membership function for each class. The membership func- tion is 1 for instances that belong to that class and 0 for other instances. Given a new instance, we calculate its membership for each class and select the biggest.

Multiresponse linear regression often yields good results in practice. However, it has two drawbacks. First, the membership values it produces are not proper probabilities because they can fall outside the range 0 to 1. Second, least-squares regression assumes that the errors are not only statistically independent but are also normally distributed with the same standard deviation, an assumption that is blatently violated when the method is applied to classification problems because the observations only ever take on the values 0 and 1.

A related statistical technique called logistic regression does not suffer from these problems. Instead of approximating the 0 and 1 values directly, thereby risking illegitimate probability values when the target is overshot, logistic regression builds a linear model based on a transformed target variable %\cite{Witten and Frank|.

In logistic regression the probability of classes is estimated in the following way:

$$P(1 | w_1, w_2, ... w_n) = \dfrac{1}{1 + e^(-a_0 - a_1 w_1 - ... - a_n w_n)}$$
- where $a_i$ is weight.

$$P(0 | w_1, w_2, ... w_n) = 1 - P(1 | w_1, w_2, ... w_n)$$

The goal function, which must be minimized, is:

$$\sum_{i}^{m} {(1 - x_i) log(1 - P[1 | w_1, w_2, ... w_n]) + x_i log(P[(1 | w_1, w_2, ... w_n])}$$

In the multiclass case, the training algorithm uses the one-vs-rest. One-vs-the-rest is also known as one-vs-all. This strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only $n$ classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice %\cite{McKinney}.

In comparison with Naive Bayes algorithm Logistic regression is better at Domains with many non-equally important features. Moreover, Logistic regression is as well robust to irrelevant features when regularization is properly set %\cite{kevinPMuprhy}.

There are also drawbacks, for instance, Logistic regression is really fast algorithm, still Naive Bayes is faster.

\newpage

\section{Perfomance evaluation}

TODO describe F1-score.

\newpage

\chapter{Feature Extraction}

\section{Motivation}

TODO mention that usually algorithms classify opinion as either positive or negative. My SA is more precise.

TODO say that in this work there is trade-off between classification quality and time required.

In linguistic morphology and information retrieval, stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.

Stemming programs are commonly referred to as stemming algorithms or stemmers.


Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.

In computational linguistics, lemmatisation is the algorithmic process of determining the lemma for a given word. Since the process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence (requiring, for example, knowledge of the grammar of a language) it can be a hard task to implement a lemmatiser for a new language.

In many languages, words appear in several inflected forms. For example, in English, the verb ‘to walk’ may appear as ‘walk’, ‘walked’, ‘walks’, ‘walking’. The base form, ‘walk’, that one might look up in a dictionary, is called the lemma for the word. The combination of the base form with the part of speech is often called the lexeme of the word.

Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.


The Brown Corpus was a carefully compiled selection of current American English, totaling about a million words drawn from a wide variety of sources. Every word is labaled with part of speech.

Part-of-speech tags used can be found on wikipedia.

Consider the simple case of text classification based on the presence or absence of just one word $w \in W$. Suppose we know that the word $w$ only occurs is reviews with negative opinion and never is present in reviews with positive opinion. This gives us confidence that any message containing $w$ is negative. This approach can be generalized to the probability of a message feature vector occurring in the message.

The entities, I need to classify, are text messages that are given in the form of strings. Raw strings are not convenient objects to handle by classifiers. Most machine learning algorithms can only classify numerical objects or otherwise require a distance metric or other measure of similarity between the objects. Moreover, string by itself doesn't represent a polarity of the message in any way. Therefore, set of features should be extracted.

Before proceeding with machine learning we have to convert all messages to numerical vectors called \textit{feature vectors}, and then classify these vectors. The simplest example of a feature vector is the vector of the numbers of occurrences of all the words in the dictionary in a message. The problem wih this approach is that not all the words affect a sentiment. In particular there are so-called STOP words which are completely useless in this respect. This causes significant performance degradation. Additionaly, this approach does not analyze the context of the word, which causes prediction precission to be lower.

Extraction of features usually means that some information from the original message is lost. On the other hand, the way feature vector is chosen is crucial for the performance of the filter. There is a trade-off between prediction quality and training speed. Usually, the difference between performance of a really detailed model and a reasonably simple model is not significant. Yet the training speed of a simple model is considerably swifter. Therefore, in most practical applications the most basic vector of word frequencies or its modification is used.

There are plethora of machine learning algorithms which are suitable for review classification task. Choosing the best algorithm may improve a quality of prediction. In practice, however, it is much more important what features are chosen for classification than what classification algorithm is used. If the features are chosen badly, then any machine learning algorithm will fail to classify data correctly no matter how good it is. A reasonable choise of features can make classification both precise and quick.

\newpage

\section{Important Words Selection}

TODO mention that there are more than 100.000 unique words - therefore using bigrams and trigrams is certainly feasible, but hurts performance severely. Or whatever but avoid using bigrams and trigrams

As already was mentioned in the previous section, selecting all the words as features is not very practical. Yet in performance comparison I will use this approach as well because it can be a baseline for other extraction functions quality. In case of using all the words let us define a word dicionary $D \subseteq W$. Let us suppose $|D| = k|$. Then for matrix $X$ which is $m \times n$ dimensional, now $n = k$. Now $x_{ij} \in X$ represents number of occurances of the word $d_j \in D$ in the review $r_i \in R$.

Now let us consider extracting a smaller dictionary. The problem is to decide which words to include not to lose important features. As it turns out adjectives used most often define an opinion. There is almost no use in adverbs, because of a domain. It is possible that in restaurant reviews there are lots of them, yet for baby products it is not.  Also I am going to use link verbs. Common link verbs are: be, appear, look, seem, become, get. For instance, in the sentence "My baby seemed happy". Another reasonable idea is including some verbs such as love, like, enjoy, recommend, etc.

Lemmatisation vs Stemming
Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.

\newpage

\section{Context Mining}

In the previous section I described in detail the process of words selection. However, before begining classification there is still a work to be done. Consider the following example: "My baby likes this toy", "My baby doesn't like this toy". Using our previous approach there is no difference between those two sentences in classification matrix. But obviously there is a significant difference - pollarity is exactly the opposite. One solution would be to include "not" in the dictionary and hope that classifier will give it significant negative wage. The issue with this approach though is that in more complex reviews, a negation can bear positive sense. Consider an example "I really like this cradle. It looks nice and my baby does not cry as often as before". Therefore, another solution should be found. I suggest using antonyms.

\newpage

\chapter{Effectiveness of trained models}

Words, tables, graphs, pictures, code.

Multinomial seems to work better then ovr for Linear regression

Just remember that in case that you use the n-grams framework, the number of n should not be too big. Particularly in Sentiment Analysis you will see that using 2-grams or 3-grams is more than enough and that increasing the number of keyword combinations can hurt the results. Moreover keep in mind that in Sentiment Analysis the number of occurrences of the word in the text does not make much of a difference. Usually Binarized versions (occurrences clipped to 1) of the algorithms perform better than the ones that use multiple occurrences.




\newpage

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

Words.

\newpage

\addcontentsline{toc}{chapter}{References}

\begin{thebibliography}{9}

\bibitem{Chris Manning and Hinrich} Manning, Chris and Hinrich Sch{\"u}tze. \textit{Foundations of Statistical Natural Language Processing}. Cambridge [England]: MIT Press, 1999.

\bibitem{Wake} Wake, Lisa. \textit{NLP : principles in practice}. St. Albans, Herts [England]: Ecademy Press, 2010.

\bibitem{kevinPMuprhy} Murphy, Kevin. \textit{Machine Learning A Probabilistic Perspective}. Cambridge, Massachusetts [USA]: Massachusetts Institue of Technology, 2012.

\bibitem{Jure Leskovec, Anand Rajaraman, Jeff Ullman} Leskovec, Jure, Anand Rajaraman, and Jeff Ullman. \textit{Mining of Massive Datasets SECOND EDITION}. Cambridge [England]: Cambridge University Press, 2014.

\bibitem{Witten and Frank} Witten, Ian, and Eibe Frank. \textit{Data Mining: Practical Machine Learning Tools and Techniques THIRD EDITION}. Burlington [USA]: Morgan Kaufmann Publications, 2010.

\bibitem{McKinney} McKinney, Wes. \textit{Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython 1st Edition}. Sebastopol [USA]: 2012.

\bibitem{mic} \textit{Microsoft Naive Bayes Algorithm}. Microsoft, 2016. Web. 26 May 2016. <https://msdn.microsoft.com/en-us/library/ms174806.aspx>.

\bibitem{pos-neg} \textit{Opinion Mining, Sentiment Analysis, and Opinion Spam Detection}. University of Illinois at Chicago, Web. 26 May 2016. <http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar>.

\bibitem{potts} \textit{Sentiment Symposium Tutorial: Tokenizing}. Christopher Potts, 2011. Web. 26 May 2016. <http://sentiment.christopherpotts.net/tokenizing.html>.

\bibitem{Minqing} Minqing, Hu, and Liu Bing. \textit{Mining and Summarizing Customer Reviews}. Seattle [USA]: 2004.

\bibitem{amazon-data} Carlos Guestrin, Amazon Professor of Machine Learning. Web. 26 May 2016. <https://s3.amazonaws.com/static.dato.com/files/coursera/course-3/amazon\_baby.csv.zip>.

\bibitem{turney} Turney, Peter. \textit{Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews}. Philadelphia, Pennsylvania [USA]: 2002.

\bibitem{Bing} Bing, Liu, Hu Minqing, and Junsheng Cheng. \textit{Opinion Observer: Analyzing and Comparing Opinions on the Web.}. Chiba [Japan]: 2005.

\end{thebibliography}

\end{document}