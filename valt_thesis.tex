\documentclass[12pt]{report}

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{srcltx}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}

\fancypagestyle{plain}{}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\renewcommand{\headrulewidth}{0 pt}
\rhead{\thepage}
\setlength{\headwidth}{483 pt}
\setlength{\headheight}{0.8 pt}

\paperheight 297mm \paperwidth 220mm \oddsidemargin -7mm
\topmargin 0mm \textwidth 170mm \textheight 220mm \marginparsep
0mm \marginparwidth 0mm \headheight 0mm \headsep 0mm \footskip 5mm
\parskip 2pt

\DeclareMathOperator{\Bool}{Bool}
\newcommand{\Wr}{\mathop{\font\xx=cmsy10 at 18pt \hbox{\xx \char111}}}
\def\thnumbering{chapter}
\def\equa{\Leftrightarrow}
\def\x{{\mathcal{X}}}
\def\b{{\mathcal{B}}}

\newcommand{\bb}[1]{{\bf #1}}
\newcommand{\wsp}{\hspace{6pt}}
\newcommand{\re}{\mathbb{R}}
\newcommand{\Loss}{\mathcal{L}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]

\makeatletter
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}{-\baselineskip}{-5pt}{\bf}}
\makeatother


\begin{document}


\renewcommand{\bibname}{References}
\setcounter{tocdepth}{1}

\setcounter{page}{2}

\large

\thispagestyle{empty}
\tableofcontents

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

The problem of text analysis has been around since the end of the previous century and has since evolved to numerous forms. Other names for this problem are text mining and text analytics. Labor-intensive manual text mining approaches first surfaced in the mid-1980s, but technological advances have enabled the field to advance during the past decade. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and a plethora of others.
The term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation.

Sentiment analysis, which refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials, is the most crucial problem for businesses related to social media and market advertisement. An average client may produce about 100 messages per day. What is more, sometimes sentiment data is generated by web crawlers which aggregate data from various sources. Given the amount of data those enterprises poses, it is almost infeasible to process all the messages manually. Therefore, automatic text analytics is indispensable. Even those methods with low precision can generate profits.

A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level - whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Advanced, "beyond polarity" sentiment classification looks, for instance, at emotional states such as "angry," "sad," and "happy." In this work only the basic task is considered.

Existing approaches to sentiment analysis can be grouped into three main categories: knowledge-based techniques, statistical methods, and hybrid approaches. Knowledge-based techniques classify text by affect categories based on the presence of unambiguous affect words such as happy, sad, afraid, and bored. Some knowledge bases not only list obvious affect words, but also assign arbitrary words a probable "affinity" to particular emotions. Statistical methods leverage on elements from machine learning such as latent semantic analysis, support vector machines, "bag of words" and Semantic Orientation - Pointwise Mutual Information.

In this paper I consider using text analysis and statistical methods to perform sentiment analysis of the data refered to specific domain. I leverage different common approaches and suggest improvements to them for solving the particular problem I work on. Moreover, I analyze some of the common problems like feature vector selection. Finally, I provide a performance comparison of the detailed algorithms in practice.

\newpage


\chapter{Statement of the Problem}

\section{Scope and Applications}

A marketing campaign is efforts of a company or a third-party marketing company to increase awareness for a particular product or service, or to increase consumer awareness of a business or organization. It has a limited duration. A marketing campaign consists of specific activities designed to promote a product, service or business. A marketing campaign is a coordinated series of steps that can include promotion of a product through different mediums (television, radio, print, online) using a variety of different types of advertisements. The campaign doesn't have to rely solely on advertising, and can also include demonstrations, word of mouth and other interactive techniques.

After a particular campaign is finished, the business is interested in aggregating people's opinion about the products which were being promoted. The vast majority of the reviews come from social media and shoping websites. Basically a review is a short text which is either positive or negative. As was mentioned before, because of large quantities of customers and review sources, manual review analysis is not feasible. The solution is to use an automated review analysis. A classificator can be built on the date that have been already analysed by humans. The larger the corpus the better.

In general, the problem of sentiment analysis depends heavily on the application's domain and can benefit from additional metadata available along with the text data. For example, in case of reviews related to political domain, the analyst might expect some amount of sarcasm and indirect implications. Moreover, a language which is used also plays important role; similar expressions in different languages have different shades of meaning. Therefore, it often is beneficial to target a specific domain and a specific language in text sentiment analysis problem.

In this paper I am going to target customer reviews of baby products in English available on amazon website, the biggest retailer in the world. The data is available at https://s3.amazonaws.com/static.dato.com/files/coursera/course-3/amazon\_baby.csv.zip.
This data contains 183532 labeled products reviews. The polarity grade is 1-5. 1 - means the most negative review and 5 - the most positive.

\section{Definitions}

Let us denote a set of all possible words $W$ and a set of all possible permutations G.
And let $R$ be a set of all the reviews available. So $r \in R$ - product review.
$R = \{(w, g): w \in W, g \in G\}$.
$P$ - review polarities set. Each review has corresponding polarity $p=\{1,2,3,4,5\}, p \in P$.
$|R| = |P| = m$.

$X$ feature matrix for machine learning. $X$ is an $m\times n$ matrix, where $m$ is a number of training samples and $n$ is a feature vector cardinality.

$Y$ - label vector for machine learning. In this case it is a mapped polarity of a review. $Y=\{-1,1\}$, where -1 correspond to negative opinion and 1 - to the positive. $|Y| = m$.

$X_{train} \subseteq X$ - training data.

$X_{test} \subseteq X$ - test data.

$Y_{train} \subseteq Y$ - training labels.

$Y_{test} \subseteq Y$ - test labels.

$extr: R \rightarrow X$ - feature extraction function. Given a review $r$, this functions maps it to the feature vector $x$.

$pol: P \rightarrow Y$ - polarity map function. Maps polarity to binary vector. 

$f: X \rightarrow Y$ - classification function. Given a feature vector $x$, this function predicts its label $y$.

Now text analysis process can be defined. The sets $R$ and $P$ are available.
Firstly, an extraction function $extr$ must be defined. After feature extraction process we should be able to form a feature matrix $X$. Also the representation of the polarities set $P$ is changed by applying a map function $pol$. Hence, $X$ and $Y$ are defined.

At this moment, I shall split the data into training and cross-validation set. Or in other words training and test data. Now $f$ must be found by a classification algorithm on the training set.

The ultimate goal is to obtain a decision function $F : R \rightarrow \{extr, f\}$ that would minimize $\dfrac1 {m_{test}} \sum_{i=1}^{m_{test}} (f(x_{i})- y_{i})^2$ for training data. 

\newpage

\chapter{Feature Extraction}

\section{Motivation}

Consider the simple case of text classification based on the presence or absence of just one word $W$. Suppose we know that the word $W$ only occurs is spam messages. This gives us confidence that any message containing $W$ is spam. This approach can be generalized to the probability of a message feature vector occurring in the message.

The entities, I need to classify, are text messages that are given in the form of strings. Raw strings are not convenient objects to handle by classifiers. Most machine learning algorithms can only classify numerical objects or otherwise require a distance metric or other measure of similarity between the objects. Moreover, string by itself doesn't represent a polarity of the message in any way. Therefore, set of features should be extracted.

Before proceeding with machine learning we have to convert all messages to numerical vectors called \textit{feature vectors}, and then classify these vectors. The simplest example of a feature vector is the vector of the numbers of occurrences of all the words in the dictionary in a message. The problem wih this approach is that not all the words affect a sentiment. In particular there are so-called STOP words which are completely useless in this respect. This causes significant performance degradation. Additionaly, this approach does not analyze the context of the word, which causes prediction precission degradation.

Extraction of features usually means that some information from the original message is lost. On the other hand, the way feature vector is chosen is crucial for the performance of the filter. If the features are chosen so that there may exist a spam message and a legitimate mail with the same feature vector, then any machine learning algorithm will make mistakes no matter how good it is. A wise choice of features will make classification much easier while also fast. In most practical applications the most basic vector of word frequencies or its modification is used.

Note that at the stage of feature selection it is possible to include the features from the available metadata along with features from message text. In practice, however, it is much more important what features are chosen for classification than what classification algorithm is used.

Now let us consider those machine learning algorithms that require distance metric or scalar product to be defined on the set of messages. There does exist a suitable metric (edit distance), and there is a nice scalar product defined purely for strings [2], but the complexity of the calculation of these functions is sometimes too restrictive to use them in practice. So in this work we shall simply extract the feature vectors and use the distance/scalar product of these vectors.

\newpage

\section{Important Words Selection}

Bla bla bla

\newpage

\section{Context Mining}

Bla bla bla

\newpage

\chapter{Classification Techniques}

\section{k Nearest Neighbors Classifier}

$k$ Nearest Neighbors (or $k$-NN) is a modification of the classical Nearest Neighbors algorithm. The idea behind this classifier is to first define a metric on feature vectors and then classify the message according to classes of $k$ nearest messages in the training set. The metric is often chosen to be Euclidean, but Hamming or $l_p$ can also be used for this purpose.

\textbf{Training process}

\begin{enumerate}
        \item Store feature vectors of training messages in two sets $L$ and $S$.
\end{enumerate}

\textbf{Classification process}

\begin{enumerate}
	\item For message with feature vector $x$ determine $k$ nearest neighbors from messages in the training set. If there are more legitimate messages among them, classify $x$ as legitimate message, otherwise classify as spam.
\end{enumerate}

Since the algorithm does not require any preprocessing of the training dataset, the training process is trivial. The classification process, however, requires calculation of distances to all messages in the training set, and for feature vectors of length $m$ the most trivial implementations take $O(mn)$ time for set of $n$ messages in case of Hamming or $l_p$ metrics. Performing indexing on the training set can decrease the running closer to $O(n)$ \cite{Tretyakov}. However, if the size of the set increases over time, the algorithm might not be feasible in practice for certain applications.

The $k$-NN classifier is widely applicable in general classification problems, partially because it is one of so called \textit{universally consistent} rules. Consider the training set $s_n$ of $n$ samples, and let use denote the $k$-NN classifier over that set as $f_{s_n}$. Similar to Bayesian classifier, we can determine the average risk $R(f_{s_n})$ of the classifier. The risk value is always greater than or equal than the Bayesian risk $R_*$ (recall that Bayesian classifier is optimal in this sense), however for large values of $n$ $R(f_{s_n})$ will be close to $R_*$.

\begin{definition}
	A classification rule is called consistent if the expectation of the average risk $E(R_n)$ converges to the optimal (Bayesian) risk $R_*$ as $n$ goes to infinity:
	$$E(R_n) \xrightarrow[n \rightarrow \infty]{} R_*$$.
	The classification rule is called universally consistent if it is consistent for any distribution of $(x, c)$.
\end{definition}

\begin{theorem}[Stone, 1977]
	If $k \rightarrow \infty$ and $\frac{k}{n} \rightarrow 0$, then $k$-NN classification rule is universally consistent.
\end{theorem}

Consistency of $k$-NN rule allows to increase the quality by increasing the size of the training set. Stone theorem guarantees that as the size of the training set increases with constant value of $k$, the selection of messages for the training set does not matter. In addition to this, small values of $k$ prevent quadratic complexity $O(n^2)$ when computing nearest neighbors.

Despite theoretical results, $k$-NN classifiers are performing worse than competition in practice in spam classification and are computationally expensive.

\newpage

\section{Naive Bayes Classifier}

Consider the simple case of text classification based on the presence or absence of just one word $W$. Suppose we know that the word $W$ only occurs is spam messages. This gives us confidence that any message containing $W$ is spam. This approach can be generalized to the probability of a message feature vector occurring in the message.

Suppose we have two classes $L$ and $S$ corresponding to legitimate and spam messages, and that there is a known probability distribution of feature vectors $P(x | c), c \in \{L, S\}$. In general it is hard to define such distribution, but it is often possible to provide an approximation. What we need to obtain is the class that the given message belongs to, or the probability $P(c | x)$. This can be done using the Bayes' formula

$$P(c | x) = \dfrac{P(x | c) P(c)}{P(x)} = \dfrac{P(x | c) P(c)}{P(x | L) P(L) + P(x | S) P(S)}$$.

where $P(x)$ is the a-priori probability of a message with feature vector $x$ and $P(c)$ is the probability of class $c$, i.e. the probability that any given message belongs to $c$. Given the values $P(c)$ and $P(x | c)$ for $c \in \{L, S\}$ one can calculate the probability $P(c | x)$ which can then be used in a classification rule.

The most basic classification rule is to classify message to the category with bigger probability.

\begin{definition}
	Maximum a-posteriori probability (MAP) rule: if $P(S | x) > P(L | x)$ then classify $x$ as spam, otherwise classify as legitimate message.
\end{definition}

The MAP rule can be transformed to

\begin{center}
	If $\dfrac{P(x | S)}{P(x | L)} > \dfrac{P(L)}{P(S)}$ then classify $x$ as spam, otherwise as legitimate message.
\end{center}

The ratio $\dfrac{P(x | S)}{P(x | L)}$ is known as the \textit{likelihood ratio} and is denoted as $\Lambda(x)$.

This approach can be too simplistic for certain applications. For example, in case of e-mail spam filtering, false positives (classifying legitimate message as spam) are usually much more unwanted than false negatives (classifying spam as legitimate message). The following generalization allows to take such restrictions into account.

\begin{definition}
	A cost function $\Loss(c_1, c_2)$ denotes the cost of misclassifying a message of class $c_1$ as the one belonging to class $c_2$.
\end{definition}

It is natural to put $\Loss(L, L) = \Loss(S, S) = 0$, but in general it might not be the case.

Then we can express the expected risk of classifying a message $x$ belonging to class $c$ in the above terms.

\begin{definition}
	The function $R(c | x) = \Loss(S, c) P(S |x) + \Loss(L, c) P(L | x), x \in M, c \in \{L, S\}$ is called the risk function.
\end{definition}

Now we can define a natural classification rule in terms of expected risk.

\begin{definition}
	Bayes' classification rule: if $R(S | x) < R(L | x)$ then classify $x$ as spam, otherwise as legitimate message \cite{Kecman}.
\end{definition}

It can be shown that Bayesian classifier $f$ minimizes the average risk

$$R(f) = \int \Loss(c, f(x)) dP(c, x) = P(L) \int \Loss(L, f(x))dP(x | L) + P(S) \int \Loss(S, f(x))dP(x | S)$$

so in this sense Bayesian classifier already is optimal \cite{Tretyakov}.

Naturally, the loss of classifying the message correctly is zero, thus $L(S, S) = L(L, L) = 0$. Then the Bayes's classification rule can be rewritten as

\begin{center}
	If $\Lambda(x) > \lambda \dfrac{P(L)}{P(S)}$ classify as spam otherwise as legitimate message.
\end{center}

Here $\lambda = \dfrac{\Loss(L, S)}{\Loss(S, L)}$ is the additional parameter that specifies the risk of misclassifying legitimate messages as spam. As the value of $\lambda$ increases, the classifier produces fewer false positives.

While the classification process is straightforward, the practical applications of Bayes's classifier are limited by our ability to approximate the a-priori probabilities $P(x | c)$ and $P(c), c \in \{L, S\}$ from the training data. Therefore, while the Bayes's classifier is optimal in the sense of minimizing the loss of classification for given a-priori probabilities, the quality of spam detection depends on the feature selection and approximation of these probabilities.

$P(L)$ and $P(S)$ can be easily approximated by the ratio of legitimate and spam messages respectively. $P(x | c)$ is non-trivial and depends on the contents of selected feature vector. Consider the simplest case where the feature vector $x_w$ is $1$ if the message contains $w$ and $0$ otherwise. Then the probability $P(x_w = 1 | S)$ can be approximated by the ratio of spam messages containing $w$ to the ratio of all spam messages in a training set. This is sufficient to be used by the Bayes's classifier, so we can outline the training and selection process for this model.

\textbf{Training process}

\begin{enumerate}
	\item Calculate probabilities $P(c)$, $P(x_w = 0 | c), P(x_w = 1 | c), c \in \{L, S\}$.
	\item Using Bayes's formula calculate $P(c | x_w = 0)$ and $P(c | x_w = 1)$.
	\item Calculate $\Lambda(x_w), x_w = 0, 1$, calculate $\lambda \dfrac{P(L)}{P(S)}$ and store these values.
\end{enumerate}

\textbf{Classification process}

\begin{enumerate}
	\item Determine feature vector $x_w$ for message $m$.
	\item Retrieve the stored value $\Lambda(x_w)$.
	\item Use Bayes's decision rule to determine class of the message.
\end{enumerate}

Now we need to generalize this classifier to include more features than just the presence of a single word. The simplest way (and a very common one) is to choose a set of most common words $w_1, w_2, \dots, w_n$ and define the feature vector $x = (x_1, x_2, \dots, x_n), x_i = $, $x_i = 1$ if the message contains $w_i$, $x_i = 0$ otherwise.

The problem with this approach is that it requires calculation and storing of all possible values of the feature vector, and there are $2^n$ such vectors, which is not feasible. A common way to remove this requirement is to assume that the individual components of the vector are independent \cite{Tretyakov}. This assumption is not formally correct, but in practice it is a good compromise between formal correctness and computational requirements. We will consider other options in later chapters.

Because of independence of features:

$$P(x | c) = \prod_{i=1}^{n}P(x_i | c)$$
$$\Lambda(x) = \prod_{i=1}^{n}\Lambda_i(x_i)$$

This classifier is known as Naive Bayesian Classifier due to assumption of independence of features. Training and classification are very simple computationally.

\textbf{Training process}

\begin{enumerate}
	\item For all $w_i \in W$ calculate and store $\Lambda_i(x_i), x = 0, 1$.
	\item Calculate and store $\lambda \dfrac{P(L)}{P(S)}$.
\end{enumerate}

\textbf{Classification process}

\begin{enumerate}
	\item Determine feature vector $x$ for message $m$.
	\item calculate $\Lambda(x)$ using the stored values $\Lambda_i(x_i)$.
	\item Use Naive Bayes's decision rule to determine class of the message.
\end{enumerate}

In terms of word selection for the feature vector, usually words that are too common or too rare are excluded. For simple cases when performance is not critical, all words form the training set can be used. In later chapters we will consider ways to select words with maximum mutual information.

Another benefit of naive Bayesian filter is that it is very easy to expand the feature vector to include additional available metadata. In case of e-mails, for example, it would be contents of e-mail headers. It is possible to include additional components either to the calculation of the a-priory probability of the vector or to combine the risk of Bayesian classifier with additional risk calculated from metadata when making a decision.

\newpage

\section{SVM Classifier}

Support Vector Machines (SVM) is a class of widely used algorithms for classification and regression developed by V. Vapnik. The theoretical foundation of SVM is the Statistical Learning Theory that gives certain guarantees of performance. Let us consider classification problem with SVM for linearly separable data.

SVM works in a similar manner to perceptron in terms of finding a linear boundary that separates test data according to their classes. However, the purpose of SVM is not to find any of these boundaries if they exist, but to find the maximal margin separating hyperplane, for which the distance to the closest training sample is maximal.

\begin{definition}
	Let $X = \{(x_i, c_i)\}$, $x_i \in \re^n$, $c_i \in \{-1, +1\}$ be the set of training samples. Suppose $(w, b)$ is a separating hyperplane $sign(w^T x_i + b) = c_i$ for all $1 \leq i \leq k$. The margin $m_i$ of a training sample $(x_i, c_i)$ with respect to the separating hyperplane is the distance from $x_i$ to the hyperplane
	
	$$m_i = \dfrac{|w^T x_i + b|}{||w||}$$.
	
	The margin $m$ of the separating hyperplane for training set $X$ is the smallest margin of an instance in the training set
	
	$$m = \min_i m_i$$.
	
	The maximal margin separating hyperplane for training set $X$ is the separating hyperplane with maximal margin with respect to the training set \cite{Tretyakov}.
\end{definition}

Because the hyperplane given by parameters $(x, b)$ is the same as the hyperplane given by parameters $(kx, kb)$, we can safely bound our search by only considering canonical hyperplanes for which $\min_i |w^T x_i + b| = 1$. It is possible to show that the optimal canonical hyperplane has minimal $||w||$, and that in order to find a canonical hyperplane it suffices to solve the minimization problem: minimize $\frac{1}{2} w^T w$ under conditions

$$c_i(w^T x_i + b) \ge 1, i = 1, 2, \dots, k$$

The problem may be transformed to a certain dual form: maximize

$$L_d(\alpha) = \sum_{i = 1}^{k} \alpha_i - \frac{1}{2} \sum_{i, j = 1}^{k} \alpha_i \alpha_j c_i c_j x_i^T x_j$$

with respect to dual variables $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_k)$, $\alpha_i \ge 0 \forall i$ and $\sum_{i = 1}^{k} \alpha_i c_i = 0$.

This is a classical quadratic optimization problem. It mostly has a guaranteed unique solution, and there are efficient algorithms for finding this solution. Once we have found the solution $\alpha$, the parameters $(w_o, b_o)$ of the optimal hyperplane are determined as

$$w_o = \sum_{i = 1}^{k} \alpha_i c_i x_i$$,

$$b_o = \frac{1}{c_m} - w_o^T x_k$$

where $m$ is an arbitrary index for which $\alpha_m \ne 0$.

It is more-or-less clear that the resulting hyperplane is completely defined by the training samples that are at minimal distance to it. These training samples are called support vectors and thus give the name to the method. It is possible to tune the amount of false positives produced by an SVM classifier, by using the so-called soft margin hyperplane and there are also lots of other modifications related to SVM learning. Recall the training and classification process.

\textbf{Training process}

\begin{enumerate}
	\item Find $\alpha$ that solves the dual problem (maximizes $L_d$ under named
	constraints).
	\item Determine $w$ and $b$ for the optimal hyperplane and store the values.
\end{enumerate}

\textbf{Classification process}

\begin{enumerate}
	\item For message with feature vector $x$ classify it as $sign(w^T x + b)$.
\end{enumerate}

\newpage

\section{Decision trees}

bla bla bla

\newpage

\section{Boosting}

AdaBoost.

\newpage

\section{Artificial Neural Network Classifier}

Artificial neural networks (ANN) is a family of models inspired by biological neural networks which are widely used in classification, regression and density estimation by approximating functions that can depend on a large number of inputs and are generally unknown. A neural network is a complex function that may be decomposed into smaller units called neurons and represented graphically as a network. Many functions fall under such criteria, however the most common kinds of neurons are perceptron and multilayer perceptron.

The perceptron produces a linear function of the feature vector $f(x) = w^Tx + b$ where $f(x) > 0$ for vectors of one class and $f(x) < 0$ for vectors of another class. Here $w$ is the vector of weights, or \textit{bias}, $w = (w_1, w_2, \dots, w_n)$. This vector will be determined by the training process.

If we denote the classes by number -1 and +1, we can use $d(x) = sign(w^Tx + b)$ as decision function. This allows us to represent the decision function graphically as a neuron with $n$ inputs and a single output. A system of one perceptron is an example of the simplest neural network.

Suppose the feature vector is two-dimensional, $x \in \re^2$. Then we can represent these feature vectors as points on the plane. Then the decision function can be represented as a line dividing the plane in two parts, each corresponding to one of the classes. Similarly, the decision boundary for three-dimensional feature vectors is a plane, etc. In general, for $n$-dimensional feature vector the decision boundary is a $n$ - dimensional hyperplane.

The learning process for a perceptron is iterative. The initial values of parameters $(w_0, b_0)$ can be arbitrary, as they are updated on each iteration. On the $k$-th iteration of the algorithm a training sample $(x, c)$ is chosen such that the current decision function does not classify it correctly, i.e. $sign(w_k^Tx + b_k) \ne c$. Then the parameters $(w_k, b_k)$ are then updated according to the rule:

$$w_{k+1} = w_k + cx$$,
$$b_{k+1} = b_k + c$$.

The algorithm terminates when a decision function that correctly classifies
all training set is found. If the training set is linearly separable, the perceptron algorithm converges. It is known as Perceptron Convergence Theorem proven by Frank Rosenblatt in 1962 \cite{Cristianini}. If, however, the set is not linearly separable, the algorithm will never converge. In this case it is possible to still use the perceptron, but the training loop needs to stop when the number of misclassification becomes small.

We can now outline the training and classification phases of the perceptron.

\textbf{Training process}

\begin{enumerate}
        \item Initialize the values of $w$ and $b$ with random values or 0.
	\item Find a sample from the training set $(x, c)$ such that $sign(w^Tx + b) \ne c$. If there are no such samples, terminate as the training is completed and all training samples are being classified correctly, else proceed to the next step.
	\item Update $(w, b)$ with new values $w := w + cx$, $b := b + c$ and go to the previous step.
\end{enumerate}

\textbf{Classification process}

\begin{enumerate}
	\item For message with feature vector $x$ classify it as $sign(w^Tx + b)$.
\end{enumerate}

As mentioned before, perceptrons can be combined in multiple layers to form \textit{multilayer perceptrons} which are non-linear classifiers. Neurons of the first layer which takes in the input parameters are called \textit{input neurons}, similarly neutrons of the last layer which provide the function result value are called \textit{output neurons}. All layers between input and output are called \textit{hidden layers}.

Each neuron in the networks is similar to a perceptron: for input vector $x = (x_1, x_2, \dots, x_n)$ it calculates output value by the formula

$$o = \phi(\sum_{i = 1}^{n}w_i x_i + b)$$

where $w_i$ and $b$ are weights and bias of the neuron respectively, $\phi$ is a nonlinear function that approximates binary output of the perceptron. Most often $\frac{1}{1 + e^{ax}}$ or $\tanh(x)$ are used as $\phi$ as they tend to give a good approximation while being mathematically convenient.

Like in the case of a single perceptron, training of a neural network is searching for the values of weights and biases for all neurons that minimize the output error. Let us denote $f(x)$ as the output of the neural network. Then for training samples $(x_i, c_i), 1 \leq i \leq k$ the training has to minimize the \textit{total training error}

$$E(f) = \sum_{i = 1}^{k}|f(x_i) - c_i|^2$$.

An iterative algorithm can be used to perform this minimization. The most common one is the gradient descent which in case of neural networks is called \textit{error backpropagation}. The theory of backwards propagation of errors is well developed and has many implementations in practice \cite{Haykin}.

The main reason to use multiple layers of neurons is that the multilayer neural network is a non-linear classifier. As a result, they are applicable for tasks with training data that is not linearly separable, particularly when the number of features is relatively small. However, in case of spam detection with multiple words being used as features the data is often linearly separable, thus using neural network will have no noticeable benefits over a simple perceptron.

Performance of the neural network is proportional to the number of neurons. Thus, the large number of features directly impacts performance as it translates to increased number of input neurons and thus the complexity of the network in total. In practice the number of features would have to be more strictly limited than in case of a perceptron, which for spam detection means the trade-off between non-linear decision boundaries and the amount of information loss.

Because of the above reasons and due to the large number of parameters that require tuning. the multilayer perceptron is hard to use in practice for spam detection. It has been successfully used for that purpose \cite{Tretyakov}, but it is not easily applicable in general case as it is hard to reconfigure. For the purposes of this paper we shall focus on a simple perceptron.

\newpage

\chapter{Performance Comparison}

Words, tables, graphs, pictures, code.

\newpage

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

Words.

\newpage

\addcontentsline{toc}{chapter}{References}

\begin{thebibliography}{30}

\bibitem{Tretyakov} Konstantin Tretyakov. Machine Learning Techniques in Spam Filtering. Institute of Computer Science, University of Tartu. Data Mining Problem-oriented Seminar, MTAT.03.177, May 2004, pp. 60-79.

\bibitem{Kecman} V. Kecman. Learning and Soft Computing. 2001, The MIT Press.

\bibitem{Haykin} S. Haykin. Neural Networks: A Comprehensive Foundation. 1998, Prentice
Hall.

\bibitem{Cristianini} N. Cristianini, J. Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. 2003, Cambridge University Press. http://www.support-vector.net

\bibitem{Carreras} Xavier Carreras, Lluis Marquez. Boosting Trees for AntiSpam
Email Filtering. TALP Research Center, LSI Department, Universitat Politecnica de Catalunya.

\end{thebibliography}


\end{document}